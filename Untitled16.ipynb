{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8022ef19-f2ef-4308-98d7-f58f0526c3e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brier\\AppData\\Local\\Temp\\ipykernel_13280\\4236563646.py:91: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=32, verbose=0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=4.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(optimizer\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m], activation\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     93\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m grid_result \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Use the best hyperparameters\u001b[39;00m\n\u001b[0;32m     97\u001b[0m best_model \u001b[38;5;241m=\u001b[39m create_model(optimizer\u001b[38;5;241m=\u001b[39mgrid_result\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m], activation\u001b[38;5;241m=\u001b[39mgrid_result\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:833\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[0;32m    821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[1;32m--> 833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:345\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    343\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    346\u001b[0m         (\n\u001b[0;32m    347\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    349\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[0;32m    350\u001b[0m     )\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups):\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=4."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "# Create the csv file\n",
    "data = {'Close': [10, 20, 30, 40, 50],\n",
    "        'Volume': [100, 200, 300, 400, 500],\n",
    "        'Market Cap': [1000, 2000, 3000, 4000, 5000],\n",
    "        'Open': [5, 15, 25, 35, 45],\n",
    "        'High': [15, 25, 35, 45, 55],\n",
    "        'Low': [5, 10, 15, 20, 25]}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('crypto_data.csv', index=False)\n",
    "\n",
    "def train_best_model(df, scaler, best_model):\n",
    "    # Load and preprocess data\n",
    "    df = df.dropna()\n",
    "    df = df[[\"Close\", \"Volume\", \"Market Cap\", \"Open\", \"High\", \"Low\"]]\n",
    "    df = scaler.fit_transform(df)\n",
    "\n",
    "    # Define training and test sets\n",
    "    train_data = df[:int(df.shape[0]*0.8)]\n",
    "    test_data = df[int(df.shape[0]*0.8):]\n",
    "\n",
    "    # Prepare data for model\n",
    "    X_train, y_train = train_data[:,:-1], train_data[:,-1]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test, y_test = test_data[:,:-1], test_data[:,-1]\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Train the model\n",
    "    best_model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "    # Make predictions on test data\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    mse = np.mean((predictions-y_test)**2)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Load the csv file\n",
    "df = pd.read_csv(\"crypto_data.csv\")\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df = scaler.fit_transform(df)\n",
    "\n",
    "# Prepare data for model\n",
    "train_data = df[:int(df.shape[0]*0.8)]\n",
    "test_data = df[int(df.shape[0]*0.8):]\n",
    "X_train, y_train = train_data[:,:-1], train_data[:,-1]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test, y_test = test_data[:,:-1], test_data[:,-1]\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define the model\n",
    "def create_model(optimizer='adam', activation='relu', X_train=X_train):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=5, activation=activation, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# Load the csv file\n",
    "df = pd.read_csv(\"crypto_data.csv\")\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df = scaler.fit_transform(df)\n",
    "\n",
    "# Define training and test sets\n",
    "train_data = df[:int(df.shape[0]*0.8)]\n",
    "test_data = df[int(df.shape[0]*0.8):]\n",
    "\n",
    "# Prepare data for model\n",
    "X_train, y_train = train_data[:,:-1], train_data[:,-1]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test, y_test = test_data[:,:-1], test_data[:,-1]\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=32, verbose=0)\n",
    "param_grid = dict(optimizer=['adam', 'rmsprop'], activation=['relu', 'tanh'])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Use the best hyperparameters\n",
    "best_model = create_model(optimizer=grid_result.best_params_['optimizer'], activation=grid_result.best_params_['activation'])\n",
    "best_model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = np.mean((predictions-y_test)**2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "X_train, y_train = None, None  # These are not defined yet\n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=32, verbose=0)\n",
    "param_grid = dict(optimizer=['adam', 'rmsprop'], activation=['relu', 'tanh'])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Use the best hyperparameters\n",
    "best_model = create_model(optimizer=grid_result.best_params_)\n",
    "\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=32, verbose=0)\n",
    "param_grid = dict(optimizer=['adam', 'rmsprop'], activation=['relu', 'tanh'])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Use the best hyperparameters\n",
    "best_model = create_model(optimizer=grid_result.best_params_['optimizer'], activation=grid_result.best_params_['activation'])\n",
    "best_model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = np.mean((predictions-y_test)**2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Sleep for a period of time before retraining the model\n",
    "import time\n",
    "time.sleep(60*60*24) # sleep for 24 hours\n",
    "\n",
    "# Load new data\n",
    "new_data = pd.read_csv(\"crypto_data_new.csv\")\n",
    "\n",
    "# Preprocess new data\n",
    "new_data = new_data.dropna()\n",
    "new_data = new_data[[\"Close\", \"Volume\", \"Market Cap\", \"Open\", \"High\", \"Low\"]]\n",
    "new_data = scaler.transform(new_data)\n",
    "\n",
    "# Split new data into training and test sets\n",
    "new_train_data = new_data[:int(new_data.shape[0]*0.8)]\n",
    "new_test_data = new_data[int(new_data.shape[0]*0.8):]\n",
    "\n",
    "# Prepare new data for model\n",
    "new_X_train, new_y_train = new_train_data[:,:-1], new_train_data[:,-1]\n",
    "new_X_train = np.reshape(new_X_train, (new_X_train.shape[0], new_X_train.shape[1], 1))\n",
    "new_X_test, new_y_test = new_test_data[:,:-1], new_test_data[:,-1]\n",
    "new_X_test = np.reshape(new_X_test, (new_X_test.shape[0], new_X_test.shape[1], 1))\n",
    "\n",
    "# Train the model using the new data\n",
    "best_model.fit(new_X_train, new_y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Make predictions on new test data\n",
    "new_predictions = best_model.predict(new_X_test)\n",
    "\n",
    "# Evaluate the model's performance on new test data\n",
    "new_mse = np.mean((new_predictions-new_y_test)**2)\n",
    "print(\"Mean Squared Error:\", new_mse)\n",
    "\n",
    "while True:\n",
    "    # Sleep for a period of time before retraining the model\n",
    "    import time\n",
    "    time.sleep(60*60*24) # sleep for 24 hours\n",
    "\n",
    "    # Load new data\n",
    "    new_data = pd.read_csv(\"crypto_data_new.csv\")\n",
    "\n",
    "    # Preprocess new data\n",
    "    new_data = new_data.dropna()\n",
    "    new_data = new_data[[\"Close\", \"Volume\", \"Market Cap\", \"Open\", \"High\", \"Low\"]]\n",
    "    new_data = scaler.transform(new_data)\n",
    "\n",
    "    # Split new data into training and test sets\n",
    "    new_train_data = new_data[:int(new_data.shape[0]*0.8)]\n",
    "    new_test_data = new_data[int(new_data.shape[0]*0.8):]\n",
    "\n",
    "#Prepare new data for model\n",
    "\n",
    "def prepare_data(df):\n",
    "    df = df.dropna()\n",
    "    df = df[[\"Close\", \"Volume\", \"Market Cap\", \"Open\", \"High\", \"Low\"]]\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    df = scaler.fit_transform(df)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data = df[:int(df.shape[0]*0.8)]\n",
    "    test_data = df[int(df.shape[0]*0.8):]\n",
    "\n",
    "    X_train, y_train = train_data[:,:-1], train_data[:,-1]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test, y_test = test_data[:,:-1], test_data[:,-1]\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "#Train the model\n",
    "def train_best_model(X_train, y_train):\n",
    "    best_model = create_model(optimizer=grid_result.best_params_['optimizer'],\n",
    "                              activation=grid_result.best_params_['activation'])\n",
    "    best_model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "    return best_model\n",
    "\n",
    "#Make predictions\n",
    "def make_predictions(best_model, X_test):\n",
    "    predictions = best_model.predict(X_test)\n",
    "    return predictions\n",
    "\n",
    "#Evaluate model performance\n",
    "def evaluate_performance(predictions, y_test):\n",
    "    mse = np.mean((predictions-y_test)**2)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "#Load new data\n",
    "def load_data():\n",
    "    df = pd.read_csv(\"crypto_data.csv\")\n",
    "    return df\n",
    "\n",
    "#Continuously retrain and make predictions\n",
    "while True:\n",
    "    try:\n",
    "\n",
    "        # Load new data\n",
    "        df = load_data()\n",
    "\n",
    "        # Prepare new data for model\n",
    "        X_train, y_train, X_test, y_test =prepare_data(df)\n",
    "        \n",
    "        # Train the best model\n",
    "        best_model = train_best_model(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions =  make_predictions(best_model, X_test)\n",
    "        \n",
    "        # Evaluate model performance\n",
    "        evaluate_performance(predictions, y_test)\n",
    "        \n",
    "        # Sleep for a period of time before retraining the model\n",
    "        time.sleep(3600)  # sleep for 1 hour\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        time.sleep(3600)  # sleep for 1 hour\n",
    "    \n",
    "    # End the loop\n",
    "    break\n",
    "\n",
    "# Save the model\n",
    "best_model.save(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592859e1-33e9-479d-b2eb-4d366d589983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f9e30-f18f-457a-9853-479dccdc1e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
