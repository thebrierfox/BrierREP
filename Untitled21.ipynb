{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49bc8d1e-14d1-4d67-ab8e-b1e560213aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brier\\AppData\\Local\\Temp\\ipykernel_22036\\2831512418.py:66: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=32, verbose=0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(optimizer\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m], activation\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     68\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m grid_result \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Use the best hyperparameters\u001b[39;00m\n\u001b[0;32m     72\u001b[0m best_model \u001b[38;5;241m=\u001b[39m create_model(optimizer\u001b[38;5;241m=\u001b[39mgrid_result\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m], activation\u001b[38;5;241m=\u001b[39mgrid_result\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "# Create the csv file\n",
    "data = {'Close': [10, 20, 30, 40, 50],\n",
    "        'Volume': [100, 200, 300, 400, 500],\n",
    "        'Market Cap': [1000, 2000, 3000, 4000, 5000],\n",
    "        'Open': [5, 15, 25, 35, 45],\n",
    "        'High': [15, 25, 35, 45, 55],\n",
    "        'Low': [5, 10, 15, 20, 25]}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('crypto_data.csv', index=False)\n",
    "\n",
    "def train_best_model(df, scaler, best_model):\n",
    "    # Load and preprocess data\n",
    "    df = df.dropna()\n",
    "    df = df[[\"Close\", \"Volume\", \"Market Cap\", \"Open\", \"High\", \"Low\"]]\n",
    "    df = scaler.fit_transform(df)\n",
    "\n",
    "    # Define training and test sets\n",
    "    train_data = df[:int(df.shape[0]*0.8)]\n",
    "    test_data = df[int(df.shape[0]*0.8):]\n",
    "\n",
    "    # Prepare data for model\n",
    "    X_train, y_train = train_data[:,:-1], train_data[:,-1]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test, y_test = test_data[:,:-1], test_data[:,-1]\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Train the model\n",
    "    best_model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "    # Make predictions on test data\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    mse = np.mean((predictions-y_test)**2)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Load the csv file\n",
    "df = pd.read_csv(\"crypto_data.csv\")\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df = scaler.fit_transform(df)\n",
    "\n",
    "# Define the model\n",
    "def create_model(optimizer='adam', activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=5, activation=activation, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=32, verbose=0)\n",
    "param_grid = dict(optimizer=['adam', 'rmsprop'], activation=['relu', 'tanh'])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Use the best hyperparameters\n",
    "best_model = create_model(optimizer=grid_result.best_params_['optimizer'], activation=grid_result.best_params_['activation'])\n",
    "best_model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = np.mean((predictions-y_test)**2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Sleep for a period of time before retraining the model\n",
    "import time\n",
    "time.sleep(60*60*24) # sleep for 24 hours\n",
    "\n",
    "# Load new data\n",
    "new_data = pd.read_csv(\"crypto_data_new.csv\")\n",
    "\n",
    "# Preprocess new data\n",
    "new_data = new_data.dropna()\n",
    "new_data = new_data[[\"Close\", \"Volume\", \"Market Cap\", \"Open\", \"High\", \"Low\"]]\n",
    "new_data = scaler.transform(new_data)\n",
    "\n",
    "# Split new data into training and test sets\n",
    "new_train_data = new_data[:int(new_data.shape[0]*0.8)]\n",
    "new_test_data = new_data[int(new_data.shape[0]*0.8):]\n",
    "\n",
    "# Prepare new data for model\n",
    "new_X_train, new_y_train = new_train_data[:,:-1], new_train_data[:,-1]\n",
    "new_X_train = np.reshape(new_X_train, (new_X_train.shape[0], new_X_train.shape[1], 1))\n",
    "new_X_test, new_y_test = new_test_data[:,:-1], new_test_data[:,-1]\n",
    "new_X_test = np.reshape(new_X_test, (new_X_test.shape[0], new_X_test.shape[1], 1))\n",
    "\n",
    "# Train the model using the new data\n",
    "best_model.fit(new_X_train, new_y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Make predictions on new test data\n",
    "new_predictions = best_model.predict(new_X_test)\n",
    "\n",
    "# Evaluate the model's performance on new test data\n",
    "new_mse = np.mean((new_predictions-new_y_test)**2)\n",
    "print(\"Mean Squared Error:\", new_mse)\n",
    "\n",
    "while True:\n",
    "    # Sleep for a period of time before retraining the model\n",
    "    import time\n",
    "    time.sleep(60*60*24) # sleep for 24 hours\n",
    "\n",
    "    # Load new data\n",
    "    new_data = pd.read_csv(\"crypto_data_new.csv\")\n",
    "\n",
    "    # Preprocess new data\n",
    "    new_data = new_data.dropna()\n",
    "    new_data = new_data[[\"Close\", \"Volume\", \"Market Cap\", \"Open\", \"High\", \"Low\"]]\n",
    "    new_data = scaler.transform(new_data)\n",
    "\n",
    "    # Split new data into training and test sets\n",
    "    new_train_data = new_data[:int(new_data.shape[0]*0.8)]\n",
    "    new_test_data = new_data[int(new_data.shape[0]*0.8):]\n",
    "\n",
    "#Prepare new data for model\n",
    "def prepare_data(df):\n",
    "    df = df.dropna()\n",
    "    df = df[[\"Close\", \"Volume\", \"Market Cap\", \"Open\", \"High\", \"Low\"]]\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    df = scaler.fit_transform(df)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data = df[:int(df.shape[0]*0.8)]\n",
    "test_data = df[int(df.shape[0]*0.8):]\n",
    "\n",
    "X_train, y_train = train_data[:,:-1], train_data[:,-1]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test, y_test = test_data[:,:-1], test_data[:,-1]\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "return X_train, y_train, X_test, y_test\n",
    "\n",
    "#Train the model\n",
    "def train_best_model(X_train, y_train):\n",
    "    best_model = create_model(optimizer=grid_result.best_params_['optimizer'],\n",
    "    activation=grid_result.best_params_['activation'])\n",
    "    best_model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "    return best_model\n",
    "\n",
    "#Make predictions\n",
    "def make_predictions(best_model, X_test):\n",
    "    predictions = best_model.predict(X_test)\n",
    "return predictions\n",
    "\n",
    "#Evaluate model performance\n",
    "def evaluate_performance(predictions, y_test):\n",
    "    mse = np.mean((predictions-y_test)**2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "def load_data():\n",
    "    # Your code to load the data\n",
    "    df = pd.read_csv(\"crypto_data.csv\")\n",
    "    return df\n",
    "\n",
    "try:\n",
    "    df = load_data()\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while loading the data:\", e)\n",
    "\n",
    "#Continuously retrain and make predictions\n",
    "while True:\n",
    "    try:\n",
    "        # Prepare new data for model\n",
    "        X_train, y_train, X_test, y_test = prepare_data(df)\n",
    "\n",
    "        # Train the best model\n",
    "        best_model = train_best_model(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = make_predictions(best_model, X_test)\n",
    "\n",
    "        # Evaluate model performance\n",
    "        evaluate_performance(predictions, y_test)\n",
    "\n",
    "        # Sleep for a period of time before retraining the model\n",
    "        time.sleep(3600)  # sleep for 1 hour\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        time.sleep(3600)  # sleep for 1 hour\n",
    "\n",
    "    # End the loop if there is a KeyboardInterrupt\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "def connect_to_db():\n",
    "    # Your code to connect to the database\n",
    "    server = 'database.server.com'\n",
    "    database = 'DB'\n",
    "    username = 'user'\n",
    "    password = 'password'\n",
    "    conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "    return conn\n",
    "\n",
    "def close_db_connection(conn):\n",
    "    # Your code to close the database connection\n",
    "    conn.close()\n",
    "\n",
    "def get_data_from_db(conn):\n",
    "    # Your code to query the database and return the results\n",
    "    query = \"SELECT * FROM Table\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    return df\n",
    "\n",
    "def load_data():\n",
    "    conn = connect_to_db()\n",
    "    try:\n",
    "        df = get_data_from_db(conn)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while loading the data:\", e)\n",
    "    finally:\n",
    "        close_db_connection(conn)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a6e9f-b84e-438c-bd80-bcfa58427a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
